{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitchug/ALMlops/blob/main/M2_AST_05_Image_Segmentation_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A programme by IISc and TalentSprint\n",
        "### Assignment: Image Segmentation using U-Net and DeepLabv3+"
      ],
      "metadata": {
        "id": "zSstSEIv155v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "*  understand, prepare, and visualize the the dataset containing image and corresponding masked image used for segmentation\n",
        "*  understand the encoder, bottleneck, and decoder region of a U-Net architecture\n",
        "*  build and train a U-Net architecture for segmentation\n",
        "*  create a masked image (prediction)\n",
        "*  calculate the accuracy score like IoU and Dice-Score used in segmentation\n",
        "* understand and implement DeeplabV3+ architecture for segmentation"
      ],
      "metadata": {
        "id": "akVZjBjD2PtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "We will be training the model on the [Oxford-IIIT Pet](https://www.robots.ox.ac.uk/~vgg/data/pets/) dataset. This contains pet images, their classes, segmentation masks, and head region of interest. We will only use the `images` and `segmentation masks` for this experiment.\n",
        "\n",
        "The dataset consists of images of 37 pet breeds, with 200 images per breed. Each image includes the corresponding label and pixel-wise masks. The masks are class labels for each pixel. Each pixel is given one of three categories:\n",
        "\n",
        "* Class 1: Pixel belonging to the pet.\n",
        "* Class 2: Pixel bordering the pet.\n",
        "* Class 3: None of the above/a surrounding pixel."
      ],
      "metadata": {
        "id": "oLUH84qfbqIL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WBPPuGmBlDIN"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M2_AST_05_Image_Segmentation_A\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    # ipython.magic(\"wget https://cdn.iisc.talentsprint.com/AIandMLOps/Datasets/Acoustic_Extinguisher_Fire_Dataset.xlsx\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages"
      ],
      "metadata": {
        "id": "Q8AXIPH64g_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yhxJY5ubMo2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the Oxford-IIIT Pet dataset"
      ],
      "metadata": {
        "id": "aZ813E2AdvVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz"
      ],
      "metadata": {
        "id": "j-PKDj6Ux2Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the images in dataset\n",
        "\n",
        "* Images are present in `images/` directory\n",
        "* Corresponding segmenation masks are present in `annotations/trimaps/` directory"
      ],
      "metadata": {
        "id": "LPI24FDHcqiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize an image\n",
        "an_img_path = sorted(glob(\"./images/*\"))[0]        # The `glob` module is used to retrieve files and directories matching a specified pattern.\n",
        "print(f\"Path: {an_img_path}\")\n",
        "\n",
        "img = Image.open(an_img_path)\n",
        "img_arr = np.array(img)\n",
        "plt.imshow(img_arr);\n",
        "plt.title(\"Image\");"
      ],
      "metadata": {
        "id": "NAVxehtaN_KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a semantic part segmentation label image\n",
        "a_segm_img_path = sorted(glob(\"./annotations/trimaps/*\"))[0]\n",
        "print(a_segm_img_path)\n",
        "\n",
        "img = Image.open(# YOUR CODE HERE)\n",
        "img_arr = # YOUR CODE HERE\n",
        "# YOUR CODE HERE for plt.imshow\n",
        "plt.title(\"Segmentation Mask\");"
      ],
      "metadata": {
        "id": "UIcsPmKx8ZCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJaGbpopIRKW"
      },
      "source": [
        "### Load & Preprocess Images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train, validation, test Split:**\n",
        "* Save the images paths in a list, and Split the list to have images for train, validation, and test sets\n",
        "* Do the same for segmentation masks images"
      ],
      "metadata": {
        "id": "Q1dvg7lSAn8p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sj6gK1vIRKW"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "NUM_CLASSES = 3        # Class 1: Pixel belonging to the pet; Class 2: Pixel bordering the pet; Class 3: Surrounding pixel\n",
        "IMAGE_DIR = \"./images/\"\n",
        "MASK_IMAGE_DIR = \"./annotations/trimaps/\"\n",
        "\n",
        "# The below line uses the glob function from the glob module to find all the image files in the \"images/\" directory\n",
        "# The resulting list of file paths is sorted in ascending order.\n",
        "all_images = sorted([os.path.join(IMAGE_DIR, fname) for fname in os.listdir(IMAGE_DIR) if fname.endswith(\".jpg\")])\n",
        "\n",
        "# The below line of code finds all the mask files in the masks subdirectory \"annotations/trimaps/\".\n",
        "all_masks = sorted([os.path.join(MASK_IMAGE_DIR, fname) for fname in os.listdir(MASK_IMAGE_DIR) if fname.endswith(\".png\") and not fname.startswith(\".\")])\n",
        "\n",
        "# Creating a list of validation image files by selecting every 4th image from the all_images list.\n",
        "# Interval 4 is choosen intentionaly, as we have 200 images per category. So, will keep 1/4th in validation, 1/4th in test sets, remaining for training.\n",
        "val_images = all_images[::4]\n",
        "\n",
        "# Creating a list of validation mask files by selecting every 4th image from the all_masks list.\n",
        "val_masks = all_masks[::4]\n",
        "\n",
        "# Creating a list of test image files by selecting every 4th image starting\n",
        "# from the second image in the all_images list.\n",
        "test_images = all_images[1::4]\n",
        "\n",
        "# Creating a list of test mask files by selecting every 4th image starting\n",
        "# from the second image in the all_masks list.\n",
        "test_masks = # YOUR CODE HERE\n",
        "\n",
        "# Creating an empty list for the training image files & appending remaining images in it.\n",
        "train_images = []\n",
        "for i in all_images:\n",
        "    if (i not in val_images) and (i not in test_images):\n",
        "        train_images.append(i)\n",
        "\n",
        "# Creating an empty list for the training image files & appending remaining mask images in it.\n",
        "train_masks = []\n",
        "for i in all_masks:\n",
        "    if (i not in val_masks) and (i not in test_masks):\n",
        "        # YOUR CODE HERE for appending i in train_masks\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_images), len(train_images), len(val_images), len(test_images)"
      ],
      "metadata": {
        "id": "viupKu1bwxtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_masks), len(train_masks), len(val_masks), len(test_masks)"
      ],
      "metadata": {
        "id": "-y7upvhtw5IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load & Preprocess:**\n",
        "\n",
        "* Read the image using its path\n",
        "* Resize it to have size (128 x 128)\n",
        "* If its input image, then normalize the pixel values by diving it by 255\n",
        "* If its target mask image, then subtract 1 from it. This is a preprocessing step to adjust the segmentation mask's pixel values.\n",
        "    \n",
        "    In `annotations/README` file of the dataset it is mentioned that the pixels in the segmentation mask are labeled as { 'foreground' : 1, 'background' : 2 , 'Not Classified' : 3 }.\n",
        "\n",
        "    For the sake of convenience, we subtract 1 from the segmentation mask, resulting in labels that are : `{0, 1, 2}` and we will interpret these as {'pet', 'background', 'outline'}."
      ],
      "metadata": {
        "id": "X4k2sgEFBSTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The below function reads an image file and returns a preprocessed image tensor.\n",
        "# The mask argument is set to False by default, indicating that it is an image file, not a mask file.\n",
        "\n",
        "def read_image(image_path, mask=False):\n",
        "    if mask:\n",
        "        image = Image.open(image_path)                        # Open mask image               eg. shape (500, 600)\n",
        "        image = image.resize((IMAGE_SIZE, IMAGE_SIZE))        # Resize the mask image         eg. shape (128, 128)\n",
        "        arr = (np.array(image) - 1)                           # Change pixel values from {1,2,3} --> {0,1,2}\n",
        "        image = tf.convert_to_tensor(arr, dtype=tf.float32)   # Convert to tensor\n",
        "        image = tf.expand_dims(image, axis=-1)                # Add an extra dimension        eg. shape (128, 128, 1)\n",
        "    else:\n",
        "        image = Image.open(image_path)                        # Open input image              eg. shape (500, 600, 3)\n",
        "        image = # YOUR CODE HERE                              # Resize the image              eg. shape (128, 128, 3)\n",
        "        arr = np.array(image) / 255                           # Normalize pixel values to have values bw 0 & 1\n",
        "        image = # YOUR CODE HERE                              # Convert to tensor\n",
        "\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "sQYsEAQtMJSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test `read_image` function for an input image\n",
        "img = read_image(sorted(glob(\"./images/*\"))[0])\n",
        "print(f\"Shape: {img.shape} \\nMin pixel value: {img.numpy().min()} \\nMax pixel value: {img.numpy().max()}\")\n",
        "\n",
        "plt.imshow(img);"
      ],
      "metadata": {
        "id": "j8j6ROyHM6xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test `read_image` function for a mask image\n",
        "mask_img = read_image(sorted(glob(\"./annotations/trimaps/*\"))[0], mask=True)\n",
        "print(f\"Shape: {mask_img.shape} \\nMin pixel value: {mask_img.numpy().min()} \\nMax pixel value: {mask_img.numpy().max()}\")\n",
        "\n",
        "plt.imshow(mask_img);"
      ],
      "metadata": {
        "id": "Us6DmKphHbHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function `load_images` will apply `read_image` to all images in a list\n",
        "\n",
        "def load_images(image_paths, mask_paths):\n",
        "    images_tf = []\n",
        "    masks_tf = []\n",
        "    for i in range(len(image_paths)):\n",
        "        if np.array(Image.open(image_paths[i])).shape[-1] == 3:    # check input images must have 3 channels\n",
        "            images_tf.append(read_image(image_paths[i]))\n",
        "            masks_tf.append(read_image(mask_paths[i], mask=True))\n",
        "    return images_tf, masks_tf\n"
      ],
      "metadata": {
        "id": "NXUdfH5BY6hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess train, val, test sets\n",
        "\n",
        "train_images_tf, train_masks_tf = load_images(train_images, train_masks)\n",
        "val_images_tf, val_masks_tf = # YOUR CODE HERE to load images using val_images, val_masks\n",
        "test_images_tf, test_masks_tf = # YOUR CODE HERE to load images using test_images, test_masks"
      ],
      "metadata": {
        "id": "eWXL1iNLZP2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_images_tf), len(val_images_tf), len(test_images_tf)"
      ],
      "metadata": {
        "id": "D_oTYe_qZtRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(val_images_tf[0]);"
      ],
      "metadata": {
        "id": "UKDvkDqJMZLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(val_masks_tf[0]);"
      ],
      "metadata": {
        "id": "vgeHpPZ_bBpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create TensorFlow Dataset"
      ],
      "metadata": {
        "id": "ec7oJbQIDFDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare batches for training, validation, and testing."
      ],
      "metadata": {
        "id": "bI9jyr1Bhnrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(image_list, mask_list):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset = data_generator(train_images_tf, train_masks_tf)\n",
        "val_dataset = # YOUR CODE HERE to crete TF dataset using (val_images_tf, val_masks_tf)\n",
        "test_dataset = # YOUR CODE HERE to create TF dataset using (test_images_tf, test_masks_tf)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)\n",
        "print(\"Test Dataset\", test_dataset)"
      ],
      "metadata": {
        "id": "wjhb8bvAks69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_images_tf), len(val_images_tf), len(test_images_tf)"
      ],
      "metadata": {
        "id": "TglHJXG9kvB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "I6jNap8Dkw7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To Visualize an image, we need to iterate over a batch and access a particular image\n",
        "\n",
        "sample_batch = next(iter(val_dataset))\n",
        "random_index = np.random.choice(sample_batch[0].shape[0])\n",
        "sample_image, sample_mask = sample_batch[0][random_index], sample_batch[1][random_index]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(sample_image);\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"True Mask\")\n",
        "plt.imshow(sample_mask);"
      ],
      "metadata": {
        "id": "IPoeWl0vCaKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implement the U-Net model**\n",
        "Define model : With the dataset prepared, we can now build the UNet. Here is the overall architecture as shown below. A U-Net consists of an encoder (downsampler) and decoder (upsampler) with a bottleneck in between. The gray arrows correspond to the skip connections that concatenate encoder block outputs to each stage of the decoder. Let's see how to implement these starting with the encoder.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/U-Net.png\" width=800px height=500px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "9I-abjYWiCo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder\n",
        "The encoder is having repeating blocks. It's best to create functions for it to make the code modular. These encoder blocks contain two Conv2D layers activated by ReLU, followed by a MaxPooling and Dropout layer. Each stage has an increasing number of filters and the dimensionality of the features reduce because of the pooling layer."
      ],
      "metadata": {
        "id": "a6THXDFOlm7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Encoder utilities with the following three functions:\n",
        "\n",
        "* conv2d_block() - to add two convolution layers and ReLU activations\n",
        "* encoder_block() - to add pooling and dropout to the conv2d blocks. Recall that in UNet, you need to save the output of the convolution layers at each block so this function will return two values to take that into account (i.e. output of the conv block and the dropout)\n",
        "* encoder() - to build the entire encoder. This will return the output of the last encoder block as well as the output of the previous conv blocks. These will be concatenated to the decoder blocks as you'll see later."
      ],
      "metadata": {
        "id": "_Q4bcXVwlqPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Utilities\n",
        "\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size = 3):\n",
        "  '''\n",
        "  Adds 2 convolutional layers with the parameters passed to it\n",
        "\n",
        "  Args:\n",
        "    input_tensor (tensor) -- the input tensor\n",
        "    n_filters (int) -- number of filters\n",
        "    kernel_size (int) -- kernel size for the convolution\n",
        "\n",
        "  Returns:\n",
        "    tensor of output features\n",
        "  '''\n",
        "  # first layer\n",
        "  x = input_tensor\n",
        "  for i in range(2):\n",
        "    x = tf.keras.layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
        "            kernel_initializer = 'he_normal', padding = 'same')(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):\n",
        "  '''\n",
        "  Adds two convolutional blocks and then perform down sampling on output of convolutions.\n",
        "\n",
        "  Args:\n",
        "    input_tensor (tensor) -- the input tensor\n",
        "    n_filters (int) -- number of filters\n",
        "    kernel_size (int) -- kernel size for the convolution\n",
        "\n",
        "  Returns:\n",
        "    f - the output features of the convolution block\n",
        "    p - the maxpooled features with dropout\n",
        "  '''\n",
        "\n",
        "  f = conv2d_block(inputs, n_filters=n_filters)\n",
        "  p = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(f)\n",
        "  p = tf.keras.layers.Dropout(0.3)(p)\n",
        "\n",
        "  return f, p\n",
        "\n",
        "\n",
        "def encoder(inputs):\n",
        "  '''\n",
        "  This function defines the encoder or downsampling path.\n",
        "\n",
        "  Args:\n",
        "    inputs (tensor) -- batch of input images\n",
        "\n",
        "  Returns:\n",
        "    p4 - the output maxpooled features of the last encoder block\n",
        "    (f1, f2, f3, f4) - the output features of all the encoder blocks\n",
        "  '''\n",
        "  f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3)\n",
        "  f2, p2 = encoder_block(p1, n_filters=128, pool_size=(2,2), dropout=0.3)\n",
        "  f3, p3 = encoder_block(p2, n_filters=256, pool_size=(2,2), dropout=0.3)\n",
        "  f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2,2), dropout=0.3)\n",
        "\n",
        "  return p4, (f1, f2, f3, f4)"
      ],
      "metadata": {
        "id": "vnqUbicBl3QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bottleneck\n",
        "A bottleneck follows the encoder block and is used to extract more features. This does not have a pooling layer so the dimensionality remains the same. You can use the conv2d_block() function defined earlier to implement this."
      ],
      "metadata": {
        "id": "XvxWqZN4mAXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bottleneck(inputs):\n",
        "  '''\n",
        "  This function defines the bottleneck convolutions to extract more features before the upsampling layers.\n",
        "  '''\n",
        "  bottle_neck = conv2d_block(inputs, n_filters=1024)\n",
        "\n",
        "  return bottle_neck"
      ],
      "metadata": {
        "id": "98NQ7q4zmDHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder\n",
        "Finally, we have the decoder which upsamples the features back to the original image size. At each upsampling level, you will take the output of the corresponding encoder block and concatenate it before feeding to the next decoder block.\n",
        "\n",
        "### Creating Decoder  utilities with the following two functions:"
      ],
      "metadata": {
        "id": "o_WtIu20mGr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Utilities\n",
        "\n",
        "def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=3, dropout=0.3):\n",
        "  '''\n",
        "  defines the one decoder block of the UNet\n",
        "\n",
        "  Args:\n",
        "    inputs (tensor) -- batch of input features\n",
        "    conv_output (tensor) -- features from an encoder block\n",
        "    n_filters (int) -- number of filters\n",
        "    kernel_size (int) -- kernel size\n",
        "    strides (int) -- strides for the deconvolution/upsampling\n",
        "    padding (string) -- \"same\" or \"valid\", tells if shape will be preserved by zero padding\n",
        "\n",
        "  Returns:\n",
        "    c (tensor) -- output features of the decoder block\n",
        "  '''\n",
        "  u = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides = strides, padding = 'same')(inputs)\n",
        "  c = tf.keras.layers.concatenate([u, conv_output])\n",
        "  c = # YOUR CODE HERE to add a Dropout layer\n",
        "  c = conv2d_block(c, n_filters, kernel_size=3)\n",
        "\n",
        "  return c\n",
        "\n",
        "\n",
        "def decoder(inputs, convs, output_channels):\n",
        "  '''\n",
        "  Defines the decoder of the UNet chaining together 4 decoder blocks.\n",
        "\n",
        "  Args:\n",
        "    inputs (tensor) -- batch of input features\n",
        "    convs (tuple) -- features from the encoder blocks\n",
        "    output_channels (int) -- number of classes in the label map\n",
        "\n",
        "  Returns:\n",
        "    outputs (tensor) -- the pixel wise label map of the image\n",
        "  '''\n",
        "\n",
        "  f1, f2, f3, f4 = convs\n",
        "\n",
        "  c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c7 = decoder_block(c6, f3, n_filters=256, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c8 = decoder_block(c7, f2, n_filters=128, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c9 = decoder_block(c8, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2D(output_channels, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  return outputs\n"
      ],
      "metadata": {
        "id": "JLt3waN2mJgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it all together\n",
        "We can finally build the UNet by chaining the encoder, bottleneck, and decoder. We will specify the number of output channels and in this particular set, that would be 3. That is because there are three possible labels for each pixel: 'pet', 'background', and 'outline'."
      ],
      "metadata": {
        "id": "gjMycUhmmQLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "def unet():\n",
        "  '''\n",
        "  Defines the UNet by connecting the encoder, bottleneck and decoder.\n",
        "  '''\n",
        "\n",
        "  # specify the input shape\n",
        "  inputs = tf.keras.layers.Input(shape=(128,128,3,))\n",
        "\n",
        "  # feed the inputs to the encoder\n",
        "  encoder_output, convs = encoder(inputs)\n",
        "\n",
        "  # feed the encoder output to the bottleneck\n",
        "  bottle_neck = bottleneck(encoder_output)\n",
        "\n",
        "  # feed the bottleneck and encoder block outputs to the decoder\n",
        "  # specify the number of classes via the `output_channels` argument\n",
        "  outputs = decoder(bottle_neck, convs, output_channels=OUTPUT_CHANNELS)\n",
        "\n",
        "  # create the model\n",
        "  model = # YOUR CODE HERE to return a keras Model with (inputs=inputs, outputs=outputs)\n",
        "\n",
        "  return model\n",
        "\n",
        "# instantiate the model\n",
        "unet_model = unet()\n",
        "\n",
        "# see the resulting model architecture\n",
        "unet_model.summary()"
      ],
      "metadata": {
        "id": "TtYVCdd6mTX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.plot_model(model, show_shapes=False)"
      ],
      "metadata": {
        "id": "fQbXjGz0sqg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile and Train the model\n",
        "Now, all that is left to do is to compile and train the model. The loss we will use is sparse_categorical_crossentropy. The reason is that the network is trying to assign each pixel a label, just like a multi-class prediction. In the true segmentation mask, each pixel has either a {0,1,2}. The network here is outputting three channels. Essentially, each channel is trying to learn to predict a class and sparse_categorical_crossentropy is the recommended loss for such a scenario."
      ],
      "metadata": {
        "id": "lQlyHJIQmanu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure the optimizer, loss and metrics for training\n",
        "unet_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7Lz23OHFmaWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "model_history = unet_model.fit(train_dataset,\n",
        "                               epochs=EPOCHS,\n",
        "                               validation_data=val_dataset)"
      ],
      "metadata": {
        "id": "YNE7dqyd2kWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model weights\n",
        "unet_model.save_weights('unet_pets_model.weights.h5')"
      ],
      "metadata": {
        "id": "cWqvcoV7p6xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the model later, create the model with the same architecture and load the model weights\n",
        "## unet_model.load_weights('unet_pets_model.weights.h5')"
      ],
      "metadata": {
        "id": "R8gAZf5rgVAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the training and validation loss to see how the training went. This should show generally decreasing values per epoch."
      ],
      "metadata": {
        "id": "Ca1Yx9hYmlFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Learning curve from model history"
      ],
      "metadata": {
        "id": "vDZwgAYZf366"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_learning_curves(model_history):\n",
        "  acc = model_history.history[\"accuracy\"]\n",
        "  val_acc = model_history.history[\"val_accuracy\"]\n",
        "  loss = model_history.history[\"loss\"]\n",
        "  val_loss = model_history.history[\"val_loss\"]\n",
        "  epochs_range = range(EPOCHS)\n",
        "\n",
        "  fig = plt.figure(figsize=(8,5))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs_range, acc, label=\"train accuracy\")\n",
        "  plt.plot(epochs_range, val_acc, label=\"validataion accuracy\")\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs_range, loss, label=\"train loss\")\n",
        "  plt.plot(epochs_range, val_loss, label=\"validataion loss\")\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "dcLiVZDpgAHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display learning curves\n",
        "display_learning_curves(model_history)"
      ],
      "metadata": {
        "id": "IC5eqVFfqVxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make predictions"
      ],
      "metadata": {
        "id": "TTpGSAcdmrpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batch = next(iter(test_dataset))\n",
        "random_index = np.random.choice(sample_batch[0].shape[0])\n",
        "sample_image, sample_mask = sample_batch[0][random_index], sample_batch[1][random_index]"
      ],
      "metadata": {
        "id": "6lIeZ3AY5q5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = unet_model.predict(tf.reshape(sample_image, (1, 128, 128, 3)))         # shape (1, 128, 128, 3)\n",
        "out_img = np.squeeze(out)                                                    # shape (128, 128, 3)\n",
        "result = np.argmax(out_img, axis=2)                                          # shape (128, 128)\n",
        "\n",
        "plt.imshow(result);"
      ],
      "metadata": {
        "id": "XKLbXkFD8CIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Predictions"
      ],
      "metadata": {
        "id": "xyE-Kbx2MT2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference from model\n",
        "\n",
        "def infer(image_tensor, model, verbose=1):\n",
        "    # predictions from model, output shape -> (1, 128, 128, 3)\n",
        "    predictions = model.predict(np.expand_dims((image_tensor), axis=0), verbose=verbose)\n",
        "    # Shape after squeeze -> (128, 128, 3)\n",
        "    predictions = np.squeeze(predictions)\n",
        "    # Select only maximum predicted value for every pixel, output shape -> (128, 128)\n",
        "\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    return predictions.reshape(128,128,1)"
      ],
      "metadata": {
        "id": "PKjiu5Bih-pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the predictions"
      ],
      "metadata": {
        "id": "j38LNzT3dREQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(test_img, test_mask, model, verbose=1):\n",
        "    pred = infer(image_tensor = test_img, model = model, verbose=verbose)\n",
        "\n",
        "    fig = plt.figure(figsize=(8,5))\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.imshow(test_img)\n",
        "    plt.title(\"Input image\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.imshow(test_mask)\n",
        "    plt.title(\"Actual label\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.imshow(pred)\n",
        "    plt.title(\"Predicted label\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VOUW1R60sh76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on 1 test image\n",
        "\n",
        "plot_predictions(test_images_tf[0], test_masks_tf[0], model= unet_model)"
      ],
      "metadata": {
        "id": "7xkoJCEurJ2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on 5 test images\n",
        "\n",
        "for i in range(10,15):\n",
        "    plot_predictions(test_images_tf[i], test_masks_tf[i], model = unet_model)"
      ],
      "metadata": {
        "id": "qVLkMSsOtskC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute class-wise metrics:  IOU and Dice Score\n",
        "* **Intersection over union (IoU)**: It is known to be a good metric for measuring overlap between two bounding boxes or masks[Ground truth mask vs predicted mask]. If the prediction is completely correct, IoU = 1. The lower the IoU, the worse the prediction result.\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/IoU.jpg\" width=400px height=200px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "* **Dice score/coefficient**: It can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth.\n",
        "The Dice coefficient is 2 times The area of Overlap divided by the total number of pixels in both images.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Dice_score.jpg\" width=450px height=200px/>\n",
        "</center>\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "STnFVPOUm4CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute class wise IoU and Dice score for a single test image and its prediction\n",
        "\n",
        "def class_wise_metrics(y_true, y_pred):\n",
        "    class_wise_iou = []\n",
        "    class_wise_dice_score = []\n",
        "\n",
        "    smoothening_factor = 0.00001\n",
        "    for i in range(3):     # 3 -> no. of classes\n",
        "        intersection = np.sum((y_pred == i) * (y_true == i))\n",
        "        y_true_area = np.sum((y_true == i))\n",
        "        y_pred_area = np.sum((y_pred == i))\n",
        "        combined_area = y_true_area + y_pred_area\n",
        "\n",
        "        iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n",
        "        class_wise_iou.append(iou)\n",
        "\n",
        "        dice_score =  2 * ((intersection + smoothening_factor) / (combined_area + 2 * smoothening_factor))\n",
        "        class_wise_dice_score.append(dice_score)\n",
        "\n",
        "    return class_wise_iou, class_wise_dice_score"
      ],
      "metadata": {
        "id": "RLHv7FPPTSXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test `class_wise_metrics` function\n",
        "\n",
        "test_0 = test_images_tf[0]\n",
        "true_0 = test_masks_tf[0]\n",
        "pred_0 = infer(test_0, model= unet_model)\n",
        "\n",
        "class_wise_metrics(true_0.numpy(), pred_0)"
      ],
      "metadata": {
        "id": "-Wq_ivXqo0FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the metrics"
      ],
      "metadata": {
        "id": "vX2hiukP9krK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction for entire test_dataset\n",
        "test_preds = unet_model.predict(test_dataset)"
      ],
      "metadata": {
        "id": "d39a3P7atPdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds.shape"
      ],
      "metadata": {
        "id": "ohxxzB634197"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the prediction output\n",
        "test_predictions = np.argmax(test_preds, axis=3)\n",
        "test_predictions = test_predictions.reshape(-1,128,128,1)\n",
        "test_predictions.shape"
      ],
      "metadata": {
        "id": "eFLzfyWl5Naw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the class wise metrics for all test images\n",
        "cls_wise_iou_scores = []\n",
        "cls_wise_dice_scores = []\n",
        "for i in range(test_predictions.shape[0]):\n",
        "    test_i = test_images_tf[i]\n",
        "    true_i = test_masks_tf[i]\n",
        "    pred_i = test_predictions[i,:,:,:]\n",
        "    iou, dice = class_wise_metrics(true_i.numpy(), pred_i)\n",
        "    cls_wise_iou_scores.append(iou)\n",
        "    cls_wise_dice_scores.append(dice)\n",
        "\n",
        "# Take average to get the final result over the test set\n",
        "cls_wise_iou = np.array(cls_wise_iou_scores).mean(axis=0).round(2)\n",
        "cls_wise_dice_score = np.array(cls_wise_dice_scores).mean(axis=0).round(2)"
      ],
      "metadata": {
        "id": "5qrKsX9P9Ew4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the IOU for each class\n",
        "class_names = [\"pet\", \"background\", \"outline\"]\n",
        "\n",
        "for idx, iou in enumerate(cls_wise_iou):\n",
        "  spaces = ' ' * (10-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, iou))"
      ],
      "metadata": {
        "id": "zJgru7QvTtsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the Dice Score for each class\n",
        "for idx, dice_score in enumerate(cls_wise_dice_score):\n",
        "  spaces = ' ' * (10-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, dice_score))"
      ],
      "metadata": {
        "id": "6hIgld2xnJcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implement DeepLabV3+**"
      ],
      "metadata": {
        "id": "pl_a9FJ0IcJL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEPE701wIRKX"
      },
      "source": [
        "Downsampling is widely adopted in deep convolutional neural networks (CNN) for reducing memory consumption while preserving the transformation invariance to some degree.\n",
        "\n",
        "Multiple downsampling of a CNN will lead the feature map resolution to become smaller, resulting in lower prediction accuracy and loss of boundary information in semantic segmentation.\n",
        "\n",
        "DeepLabv3+ helps in solving these issues by including **atrous convolutions**. They aggregates context around a feature which helps in segmenting it better.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Atrous Convolution/Dilated Convolution**\n",
        "\n",
        "It is a tool for refining the effective field of view of the convolution. It modifies the field of view using a parameter termed ***atrous rate*** or ***dilation rate (d)***.\n",
        "\n",
        "With dilated convolution, as we go deeper in the network, we can keep the stride constant but with larger field-of-view without increasing the number of parameters or the amount of computation. It also enables larger output feature maps, which is useful for semantic segmentation.\n",
        "\n",
        "In the below figure, Atrous/Dilated Convolution has wider field of view with same number of parameters as Normal convolution. Only the pink ones will be consider, green ones will be ignored.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Dilated_Conv.jpg\" width=500px>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "#### **DeepLabv3+**\n",
        "\n",
        "Earlier version, DeepLabv3 has a problem of consuming too much time to process high-resolution images. DeepLabv3+ is a semantic segmentation architecture that improves upon DeepLabv3 with several improvements, such as adding an effective decoder module to refine the segmentation results.\n",
        "\n",
        "The below figure shows the typical architecture of DeepLabv3+. The encoder module processes multiscale contextual information by applying dilated/atrous convolution at multiple scales, while the decoder module refines the segmentation results along object boundaries.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/deeplabv3_plus_diagram.png\" >\n",
        "<br><br>\n",
        "\n",
        "Deeplabv3+ employs Aligned Xception network as its main feature extractor (encoder), although with substantial modifications. Depth-wise separable convolution replaces all max pooling procedures.\n",
        "\n",
        "The reason for using **Dilated Spatial Pyramid Pooling** is that it was shown that as the sampling rate becomes larger, the number of valid filter weights (i.e., weights that are applied to the valid feature region, instead of padded zeros) becomes smaller.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Model Playground, we can select feature extraction (encoding) network to use as either **Resnet** or EfficientNet.\n",
        "\n",
        "For our model, we use the below architecture.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/deeplabv3_plus_model.png\" width=1000px>\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "X839R080hMaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we use ResNet-50 as the backbone network, let's check the different layers present in it."
      ],
      "metadata": {
        "id": "1A9_lkz8lJod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ResNet-50 architecture for explore purpose\n",
        "res_input = keras.Input(shape=(128, 128, 3))\n",
        "resnet50 = keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor = res_input)\n",
        "\n",
        "# Layers present in ResNet-50 network\n",
        "resnet50.summary()"
      ],
      "metadata": {
        "id": "baedkm2Ymvtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above layers,\n",
        "\n",
        "- Use the low-level features from the `conv2_block3_2_relu` layer of the ResNet-50 network to fead in Decoder.\n",
        "\n",
        "- Use the features from the `conv4_block6_2_relu` layer of the ResNet-50 to fead in Dilated Spatial Pyramid Pooling module.\n",
        "\n"
      ],
      "metadata": {
        "id": "3viRYy34l-AB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a function, `convolution_block()`, to add a convolution layer, a BatchNormalization layer, and apply ReLu activation in one go."
      ],
      "metadata": {
        "id": "51JFQYafBWKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n",
        "    x = layers.Conv2D(num_filters,\n",
        "                      kernel_size=kernel_size,\n",
        "                      dilation_rate=dilation_rate,\n",
        "                      padding=padding,\n",
        "                      use_bias=use_bias,\n",
        "                      kernel_initializer=keras.initializers.HeNormal())(block_input)\n",
        "    x = # YOUR CODE HERE to add a BatchNormalization layer\n",
        "    x = keras.activations.relu(x)\n",
        "    return x  #tf.nn.relu(x)"
      ],
      "metadata": {
        "id": "swj5dBq3Nuhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create another function to perform Dilated Spatial Pyramid Pooling. Use above function to add different convolution blocks."
      ],
      "metadata": {
        "id": "oZ97G7zKDEwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ak0-MaLIRKX"
      },
      "outputs": [],
      "source": [
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "\n",
        "    # 1x1 Conv rate=1\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    # 3x3 Conv rate=6\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    # 3x3 Conv rate=12\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    # 3x3 Conv rate=18\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    # Image pooling\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(size = (dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation = \"bilinear\")(x)\n",
        "\n",
        "    # Concat\n",
        "    resultant = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "\n",
        "    return resultant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Encoder\n",
        "\n",
        "Create a function to implement the architecture for Encoder block. Use **ResNet50** pretrained on ImageNet as the backbone network. Use the features from the `conv4_block6_2_relu` layer of the backbone to fead in Dilated Spatial Pyramid Pooling module. Then return the backbone network along with encoder output."
      ],
      "metadata": {
        "id": "W877DUELcmAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Encoder(model_input):\n",
        "    # Backbone network\n",
        "    resnet50 = keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=model_input)\n",
        "    # Features from backbone network to fead in DSPP\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    # DSPP module\n",
        "    concat_out = DilatedSpatialPyramidPooling(x)\n",
        "    # 1x1 Conv\n",
        "    output = convolution_block(concat_out, kernel_size=1)\n",
        "\n",
        "    return resnet50, output\n"
      ],
      "metadata": {
        "id": "hF3LCKGtPIlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzIfZnCIRKY"
      },
      "source": [
        "### Create Decoder\n",
        "\n",
        "Create a function to implement the architecture for Decoder block. The encoder features are first bilinearly upsampled by a factor 4, and then concatenated with the corresponding low-level features (the `conv2_block3_2_relu` layer) from the network backbone that have the same spatial resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Decoder(image_size, back_network, x):\n",
        "    # Output from Encoder, upsample by 4\n",
        "    input_a = layers.UpSampling2D(size = (image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "                                  interpolation = \"bilinear\")(x)\n",
        "    # Low-level features from backbone network\n",
        "    input_b = back_network.get_layer(\"conv2_block3_2_relu\").output\n",
        "    # Add 1x1 Conv on low-level features\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    # Concat\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    # Add 3x3 Conv blocks\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "\n",
        "    # Resultant upsample by 4\n",
        "    output = layers.UpSampling2D(size = (image_size // x.shape[1], image_size // x.shape[2]),\n",
        "                            interpolation = \"bilinear\")(x)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "6xj8SRKFPM4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Model\n",
        "\n",
        "Create a function to implement DeepLabV3+ architecture."
      ],
      "metadata": {
        "id": "ZcWB2suiRFYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    # Encoder part\n",
        "    back_network, x = Encoder(model_input)\n",
        "    # Decoder part\n",
        "    x = Decoder(image_size, back_network, x)\n",
        "\n",
        "    # Output/prediction layer\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "\n",
        "    return # YOUR CODE HERE to return a keras Model with (inputs=model_input, outputs=model_output)\n"
      ],
      "metadata": {
        "id": "F55hQulMN85C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WPwmoG9IRKY"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "deeplab_model = DeeplabV3Plus(image_size = 128, num_classes = 3)\n",
        "deeplab_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wTaL2GIRKZ"
      },
      "source": [
        "### Training\n",
        "\n",
        "We train the model using sparse categorical crossentropy as the loss function, and\n",
        "Adam as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "deeplab_model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
        "                      loss = loss,\n",
        "                      metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "lHOIcWvJJipS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure the training parameters and train the model\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "deeplab_model_history = deeplab_model.fit(train_dataset,\n",
        "                                          epochs=EPOCHS,\n",
        "                                          validation_data=val_dataset)"
      ],
      "metadata": {
        "id": "4I296Wq0tRzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model weights\n",
        "deeplab_model.save_weights('deeplabv3plus_pets_model.weights.h5')"
      ],
      "metadata": {
        "id": "FeBGP5YiwwHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the model later, create the model with the same architecture and load the model weights\n",
        "## deeplab_model.load_weights('deeplabv3plus_pets_model.weights.h5')"
      ],
      "metadata": {
        "id": "ZfKEuxIb6iQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display learning curves\n",
        "display_learning_curves(deeplab_model_history)"
      ],
      "metadata": {
        "id": "qOC3rLp3Ii6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Predictions"
      ],
      "metadata": {
        "id": "wHnAHqNsxuc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on 1 test image\n",
        "\n",
        "plot_predictions(test_images_tf[0], test_masks_tf[0], model= deeplab_model)"
      ],
      "metadata": {
        "id": "_LY9JR7wxuc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(test_images_tf[1], test_masks_tf[1], model= deeplab_model)"
      ],
      "metadata": {
        "id": "53-LT0j8xuc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on 5 test images\n",
        "\n",
        "for i in range(10,15):\n",
        "    plot_predictions(test_images_tf[i], test_masks_tf[i], model= deeplab_model)"
      ],
      "metadata": {
        "id": "2VwJXPbvxuc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the metrics"
      ],
      "metadata": {
        "id": "RwHvvl4XUNhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction for entire test_dataset\n",
        "\n",
        "# feed the test set to the deeplab model to get the predicted masks\n",
        "test_preds_deeplab = deeplab_model.predict(test_dataset)"
      ],
      "metadata": {
        "id": "IJe46CiE7z13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds_deeplab.shape"
      ],
      "metadata": {
        "id": "ZEozhMFK7z15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process prediction output\n",
        "test_predictions_deeplab = np.argmax(test_preds_deeplab, axis=3)\n",
        "test_predictions_deeplab = test_predictions_deeplab.reshape(-1,128,128,1)\n",
        "test_predictions_deeplab.shape"
      ],
      "metadata": {
        "id": "Ju9oHxv07z15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the class wise metrics for all test images\n",
        "deeplab_cls_wise_iou_scores = []\n",
        "deeplab_cls_wise_dice_scores = []\n",
        "for i in range(test_predictions_deeplab.shape[0]):\n",
        "    test_i = # YOUR CODE HERE\n",
        "    true_i = # YOUR CODE HERE\n",
        "    pred_i = # YOUR CODE HERE\n",
        "    iou, dice = # YOUR CODE HERE for class_wise_metrics(...)\n",
        "    deeplab_cls_wise_iou_scores.append(iou)\n",
        "    deeplab_cls_wise_dice_scores.append(dice)\n",
        "\n",
        "# Take average to get the final result over the test set\n",
        "deeplab_cls_wise_iou = np.array(deeplab_cls_wise_iou_scores).mean(axis=0).round(2)\n",
        "deeplab_cls_wise_dice_score = np.array(deeplab_cls_wise_dice_scores).mean(axis=0).round(2)"
      ],
      "metadata": {
        "id": "CkTwcOAGUNhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the IOU for each class\n",
        "class_names = [\"pet\", \"background\", \"outline\"]\n",
        "\n",
        "for idx, iou in enumerate(deeplab_cls_wise_iou):\n",
        "  spaces = ' ' * (10-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, iou))"
      ],
      "metadata": {
        "id": "j_1sW0nvVOjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the Dice Score for each class\n",
        "for idx, dice_score in enumerate(deeplab_cls_wise_dice_score):\n",
        "  spaces = ' ' * (10-len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx], spaces, dice_score))"
      ],
      "metadata": {
        "id": "n1mOjcnAVWDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare with UNet"
      ],
      "metadata": {
        "id": "CvSeGtyuWLi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot bar chart to show IoU scores for predictions from both models\n",
        "\n",
        "fig = plt.figure(figsize =(6,4))\n",
        "X = np.arange(3)\n",
        "plt.bar(X + 0.25, deeplab_cls_wise_iou, color = 'g', width = 0.25, label = 'Deeplabv3+')\n",
        "plt.bar(class_names, cls_wise_iou, color = 'b', width = 0.25, label = 'UNet')\n",
        "plt.xlabel('Class label', fontsize = 12)\n",
        "plt.ylabel('IoU score', fontsize = 12)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vDYt3G-VW-ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot bar chart to show Dice scores for predictions from both models\n",
        "\n",
        "fig = plt.figure(figsize =(6,4))\n",
        "X = np.arange(3)\n",
        "plt.bar(X + 0.25, deeplab_cls_wise_dice_score, color = 'g', width = 0.25, label = 'Deeplabv3+')\n",
        "plt.bar(class_names, cls_wise_dice_score, color = 'b', width = 0.25, label = 'UNet')\n",
        "plt.xlabel('Class label', fontsize = 12)\n",
        "plt.ylabel('Dice score', fontsize = 12)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CZxc-GgnaQil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title The encoder also returns the complete resnet50 as an output while implementing Deeplabv3+ in this notebook. In the decoder, what layer is used from that resnet 50? {run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"conv4_block6_2_relu\", \"conv2_block3_2_relu\", \"the last layer of resnet50 i.e. conv5_block3_out\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ]
}