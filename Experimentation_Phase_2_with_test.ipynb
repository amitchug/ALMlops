{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitchug/ALMlops/blob/main/Experimentation_Phase_2_with_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A programme by IISc and TalentSprint\n",
        "### Assignment: Research to Production Environment: Experimentation_Phase : 2. Pipeline_Building and Test case\n"
      ],
      "metadata": {
        "id": "PprHHREe8n5R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGLbPb_LqRJd"
      },
      "source": [
        "## Learning Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of the experiment you will be able to:\n",
        "* create custom classes required for processing  \n",
        "* implement pipeline and train the model\n",
        "* save the model\n",
        "\n"
      ],
      "metadata": {
        "id": "CgWeAK5kqRJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download the datasets\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook=\"U1_MH1_Data_Munging\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    from IPython.display import HTML, display\n",
        "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/titanic.csv\")\n",
        "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/test_titanic.csv\")\n",
        "    print(\"Data downloaded successfully\")\n",
        "    return\n",
        "\n",
        "setup()"
      ],
      "metadata": {
        "id": "ViFc50xKK-tY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "RM8x-pMDLQuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of files present"
      ],
      "metadata": {
        "id": "kLVDFpbSb1ds"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WBPPuGmBlDIN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"Experimentation_Phase_2_with_test\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn6HQH7abkyL"
      },
      "source": [
        "## Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "\n",
        "import re  # Regular expressions for text processing\n",
        "import pandas as pd  # Data manipulation and analysis\n",
        "import numpy as np  # Numerical computations\n",
        "\n",
        "# Importing functions for model training and evaluation\n",
        "from sklearn.model_selection import train_test_split  # Splitting dataset into training and testing sets\n",
        "from sklearn.metrics import accuracy_score  # Evaluating model accuracy\n",
        "from sklearn.preprocessing import StandardScaler  # Standardizing features by scaling\n",
        "\n",
        "# Importing machine learning model\n",
        "from sklearn.ensemble import RandomForestClassifier  # Random Forest classifier for classification tasks\n",
        "\n",
        "# Importing visualization library\n",
        "import matplotlib.pyplot as plt  # Plotting graphs and visualizations\n",
        "\n",
        "# Importing joblib for saving and loading models\n",
        "import joblib  # Helps in persisting trained models and scalers for future use\n",
        "\n",
        "# Importing type hinting support for function annotations\n",
        "from typing import List  # Used to specify lists in function type hints\n",
        "\n",
        "# Importing pipeline utilities\n",
        "from sklearn.pipeline import Pipeline  # Used to create machine learning pipelines\n",
        "\n",
        "# Importing base classes for creating custom transformers\n",
        "from sklearn.base import BaseEstimator, TransformerMixin  # Base classes to define custom preprocessing transformers"
      ],
      "metadata": {
        "id": "apdU8g36rm4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z-lTFTy7SfZ"
      },
      "source": [
        "### **1. Pre-Pipeline-Steps: Load, Explore and Prepare the Data Set**\n",
        "\n",
        "* Understand different features in the training dataset\n",
        "* Understand the data types of each columns\n",
        "* Notice the columns of missing values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQaogC8j7SfZ"
      },
      "source": [
        "# Load the dataset from a CSV file into a pandas DataFrame\n",
        "data = pd.read_csv(\"titanic.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "t2ZjWLAK7SfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weCr4C9J7SfZ"
      },
      "source": [
        "# Getting information about the dataset\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning and Processing\n"
      ],
      "metadata": {
        "id": "rGrVLftG7SfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 1.1 Working on \"SibSp\" & \"Parch\" columns:\n",
        "Combine columns \"SibSp\" & \"Parch\" and create another column that represents the total passengers in one ticket with the name \"family_size\". In each ticket, there might be Siblings/Spouses (SibSp =Number of Siblings/Spouses Aboard) or Parents/Children (Parch=Number of Parents/Children Aboard ) along with the passenger who booked the ticket.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "VShR1Ils7Sfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def family_size(data_frame):\n",
        "    \"\"\"\n",
        "    Compute the family size for each passenger in the Titanic dataset.\n",
        "\n",
        "    Parameters:\n",
        "    data_frame (pd.DataFrame): Input DataFrame containing 'SibSp' (siblings/spouses)\n",
        "                               and 'Parch' (parents/children) columns.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A new DataFrame with an additional 'FamilySize' column.\n",
        "    \"\"\"\n",
        "    # Create a copy of the original DataFrame to avoid modifying it directly\n",
        "    df = data_frame.copy()\n",
        "\n",
        "    # Calculate family size: Number of siblings/spouses + Number of parents/children + 1 (self)\n",
        "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
        "\n",
        "    # Return the modified DataFrame with the new column\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "ibps2QaU7Sfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the family_size function to the dataset\n",
        "data = family_size(data)\n",
        "\n",
        "# Display summary information about the DataFrame\n",
        "data.info()"
      ],
      "metadata": {
        "id": "6yEvgF4W7Sfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwcaTE2i7Sfa"
      },
      "source": [
        "### 1.2 Working on \"Cabin\" column:\n",
        "Find unique entries in the Cabin column. We can label all passengers in two categories having a cabin or not. Check the data type(use: type) of each entry of the Cabin. Convert a string data type into '1' i.e. passengers with cabin and others into '0' i.e. passengers without cabin.  Write a function for the above operation and apply it to the cabin column and create another column with the name \" Has_cabin\" containing only 0 or 1 entries.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_cabin(data_frame):\n",
        "    \"\"\"\n",
        "    Create a new feature indicating whether a passenger has a cabin or not.\n",
        "\n",
        "    Parameters:\n",
        "    data_frame (pd.DataFrame): Input DataFrame containing the 'Cabin' column.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A new DataFrame with an additional 'Has_cabin' column.\n",
        "    \"\"\"\n",
        "    # Create a copy of the original DataFrame to avoid modifying it directly\n",
        "    df = data_frame.copy()\n",
        "\n",
        "    # Define a lambda function to check if a cabin entry exists\n",
        "    # If 'Cabin' is a float (i.e., NaN), return 0 (No cabin), otherwise return 1 (Has cabin)\n",
        "    f1 = lambda x: 0 if isinstance(x, float) else 1  # Ternary expression\n",
        "\n",
        "    # Apply the lambda function to the 'Cabin' column to create the 'Has_cabin' feature\n",
        "    df['Has_cabin'] = df['Cabin'].apply(f1)\n",
        "\n",
        "    # Return the modified DataFrame\n",
        "    return df"
      ],
      "metadata": {
        "id": "N3n5k0Gg7Sfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the process_cabin function to the dataset\n",
        "data = process_cabin(data)\n",
        "\n",
        "# Display summary information about the DataFrame\n",
        "data.info()"
      ],
      "metadata": {
        "id": "Q0c_akMe7Sfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3  Working on \"Name\" column :\n",
        "Fetch titles from the name. We can map these titles with numbers and convert them into an integer by mapping with relative numbers. Use: concept of the regular expression."
      ],
      "metadata": {
        "id": "w1w5zmAa7Sfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract the title (Mr, Mrs, Miss, Master, etc.) from the passenger's name\n",
        "def get_title(passenger):\n",
        "    \"\"\"\n",
        "    Extracts the title from a given passenger's name.\n",
        "\n",
        "    Parameters:\n",
        "    passenger (str): The full name of a Titanic passenger.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted title (e.g., 'Mr', 'Mrs', 'Miss', 'Master', or 'Other').\n",
        "    \"\"\"\n",
        "    line = passenger  # Assigns the passenger's name to a variable\n",
        "\n",
        "    # Check for common titles in the name and return the appropriate title\n",
        "    if re.search('Mrs', line):  # Searches for 'Mrs' in the name\n",
        "        return 'Mrs'\n",
        "    elif re.search('Mr', line):  # Searches for 'Mr' in the name\n",
        "        return 'Mr'\n",
        "    elif re.search('Miss', line):  # Searches for 'Miss' in the name\n",
        "        return 'Miss'\n",
        "    elif re.search('Master', line):  # Searches for 'Master' in the name\n",
        "        return 'Master'\n",
        "    else:\n",
        "        return 'Other'  # Returns 'Other' for uncommon or missing titles"
      ],
      "metadata": {
        "id": "TuTBLGcx7Sfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example:\n",
        "print(get_title('Heranld Mr.'))\n",
        "data['Name'].apply(get_title)"
      ],
      "metadata": {
        "id": "hah7jtV87Sfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the get_title function to the 'Name' column to extract titles (Mr, Mrs, Miss, etc.)\n",
        "data['Title'] = data['Name'].apply(get_title)\n",
        "\n",
        "# Explanation:\n",
        "# 1. 'data[\"Name\"]' selects the 'Name' column from the dataset.\n",
        "# 2. '.apply(get_title)' applies the get_title function to each value in the 'Name' column.\n",
        "# 3. The get_title function extracts and returns the title (e.g., 'Mr', 'Mrs', 'Miss', etc.).\n",
        "# 4. The extracted title is stored in a new column named 'Title' in the DataFrame."
      ],
      "metadata": {
        "id": "EVyRV2BT7Sfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the unique values in the 'Title' column to see all distinct titles\n",
        "print(data['Title'].unique())\n",
        "\n",
        "# Get a summary of the DataFrame, including column names, data types, and non-null counts\n",
        "data.info()"
      ],
      "metadata": {
        "id": "Ov0bc38h7Sfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns from the dataset\n",
        "data.drop(labels=['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
        "\n",
        "# Explanation:\n",
        "# 1. 'data.drop()' is used to remove specific columns from the DataFrame.\n",
        "# 2. 'labels=['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin']' specifies the list of column names to drop.\n",
        "# 3. 'axis=1' indicates that columns (not rows) are being dropped.\n",
        "# 4. 'inplace=True' ensures that the changes are applied directly to the DataFrame, instead of creating a new one.\n",
        "\n",
        "# Display the summary of the DataFrame to check the remaining columns\n",
        "data.info()"
      ],
      "metadata": {
        "id": "iC9ILAYA7Sfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Pipeline-Steps**\n",
        "### Building custom class compatible with sklearn pipeline for imputation, feature mapping and any specific operation on any column.\n",
        "\n",
        "### **A. Imputation**\n",
        "\n",
        "Buiding custom Imputation class compatible with Sklearn for 'Embarked' colum imputation."
      ],
      "metadata": {
        "id": "Q1M6DZzVEvVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class embarkImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"embarked column imputer.\"\"\"\n",
        "\n",
        "    def __init__(self, variables: str):\n",
        "        \"\"\"\n",
        "        Initialize the embarkImputer class.\n",
        "\n",
        "        Parameters:\n",
        "        variables (str): The name of the column to be imputed (e.g., 'Embarked').\n",
        "\n",
        "        Raises:\n",
        "        ValueError: If 'variables' is not a string.\n",
        "        \"\"\"\n",
        "        # Ensure that 'variables' is a string (column name)\n",
        "        if not isinstance(variables, str):\n",
        "            raise ValueError(\"variables should be a list\")  # Raise error if not a string\n",
        "\n",
        "        self.variables = variables  # Assign the column name to self.variables\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
        "        \"\"\"\n",
        "        Fit the imputer by calculating the most frequent value (mode) of the column.\n",
        "\n",
        "        Parameters:\n",
        "        X (pd.DataFrame): The input data to compute the mode value from.\n",
        "        y (pd.Series, optional): The target variable, not needed for this transformer.\n",
        "\n",
        "        Returns:\n",
        "        self: The fitted transformer object.\n",
        "        \"\"\"\n",
        "        # Calculate the most frequent value (mode) of the specified column in the dataset\n",
        "        self.fill_value = X[self.variables].mode()[0]\n",
        "        return self  # Return the transformer instance (needed for compatibility with sklearn pipeline)\n",
        "\n",
        "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Apply the transformation (fill missing values with the mode value).\n",
        "\n",
        "        Parameters:\n",
        "        X (pd.DataFrame): The input data with missing values in the specified column.\n",
        "\n",
        "        Returns:\n",
        "        pd.DataFrame: A DataFrame with missing values filled.\n",
        "        \"\"\"\n",
        "        # Create a copy of the input data to avoid modifying the original DataFrame\n",
        "        X = X.copy()\n",
        "\n",
        "        # Fill the missing values in the specified column with the computed fill value\n",
        "        X[self.variables] = X[self.variables].fillna(self.fill_value)\n",
        "\n",
        "        return X  # Return the transformed DataFrame"
      ],
      "metadata": {
        "id": "RkLe65xIy5g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for the number of missing (null) values in the 'Embarked' column\n",
        "data.Embarked.isnull().sum()\n",
        "\n",
        "# Explanation:\n",
        "# 1. 'data.Embarked' refers to the 'Embarked' column in the DataFrame 'data'.\n",
        "# 2. '.isnull()' generates a boolean Series where 'True' represents missing (NaN) values in the 'Embarked' column,\n",
        "#    and 'False' represents non-missing values.\n",
        "# 3. '.sum()' counts the number of 'True' values in the boolean Series, which corresponds to the number of missing values.\n",
        "#    It adds up all the 'True' values (which are treated as 1) to return the total number of missing values in the column."
      ],
      "metadata": {
        "id": "UPOQY8qJ7ec9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the embarkImputer class for the 'Embarked' column\n",
        "emb = embarkImputer('Embarked')\n",
        "\n",
        "# Fit the imputer on the 'data' DataFrame (calculates the most frequent value for 'Embarked')\n",
        "emb.fit(data)\n",
        "\n",
        "# Apply the imputer to transform the 'data' DataFrame (fills missing values in the 'Embarked' column)\n",
        "print(len(emb.transform(data).Embarked))  # Print the length of the 'Embarked' column after transformation\n",
        "\n",
        "# Save the transformed data in a new variable\n",
        "data1 = emb.transform(data)\n",
        "\n",
        "# Print the length of the 'Embarked' column in the transformed DataFrame\n",
        "print(len(data1.Embarked))\n",
        "\n",
        "# Check and print the number of missing (null) values in the 'Embarked' column of the transformed DataFrame\n",
        "print(data1.Embarked.isnull().sum())  # This should print 0 if all missing values were filled"
      ],
      "metadata": {
        "id": "9mU0joek2Bxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.Embarked.dtypes"
      ],
      "metadata": {
        "id": "cmA-VGynVY2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.info()"
      ],
      "metadata": {
        "id": "wKSxmZXr2_C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **B. Mapping**\n",
        "\n",
        "Bulding Mapper class for mpping 'Embarked','Sex' and 'Title' columns"
      ],
      "metadata": {
        "id": "wv8wir4cE1sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mapper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Categorical variable mapper.\"\"\"\n",
        "\n",
        "    def __init__(self, variables: str, mappings: dict):\n",
        "        \"\"\"\n",
        "        Initialize the Mapper class with the specified column and mappings.\n",
        "\n",
        "        Parameters:\n",
        "        variables (str): The name of the categorical variable (column) to be mapped.\n",
        "        mappings (dict): A dictionary where keys are the unique categorical values\n",
        "                         in the column, and values are the new values to map them to.\n",
        "\n",
        "        Raises:\n",
        "        ValueError: If 'variables' is not a string.\n",
        "        \"\"\"\n",
        "        # Ensure that 'variables' is a string (column name)\n",
        "        if not isinstance(variables, str):\n",
        "            raise ValueError(\"variables should be a str\")  # Raise error if not a string\n",
        "\n",
        "        self.variables = variables  # Assign the column name to self.variables\n",
        "        self.mappings = mappings  # Assign the mapping dictionary to self.mappings\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
        "        \"\"\"\n",
        "        Fit method (no operation needed in this case, required for compatibility with the sklearn pipeline).\n",
        "\n",
        "        Parameters:\n",
        "        X (pd.DataFrame): The input data.\n",
        "        y (pd.Series, optional): The target variable, not needed for this transformer.\n",
        "\n",
        "        Returns:\n",
        "        self: The fitted transformer object.\n",
        "        \"\"\"\n",
        "        # Fit is not used for the mapper as the mapping is predefined\n",
        "        return self  # Return the transformer instance (needed for compatibility with sklearn pipeline)\n",
        "\n",
        "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Apply the transformation (map the categorical values in the column to the new values).\n",
        "\n",
        "        Parameters:\n",
        "        X (pd.DataFrame): The input data with categorical variables to be mapped.\n",
        "\n",
        "        Returns:\n",
        "        pd.DataFrame: A DataFrame with mapped categorical values in the specified column.\n",
        "        \"\"\"\n",
        "        # Create a copy of the input data to avoid modifying the original DataFrame\n",
        "        X = X.copy()\n",
        "\n",
        "        # Map the categorical values in the specified column using the provided mappings dictionary\n",
        "        # The 'map' function replaces each value in the column with its corresponding value from the mappings\n",
        "        # After mapping, the column is converted to an integer type using 'astype(int)'\n",
        "        X[self.variables] = X[self.variables].map(self.mappings).astype(int)\n",
        "\n",
        "        return X  # Return the transformed DataFrame with mapped values"
      ],
      "metadata": {
        "id": "-KVgKXzW1EcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Mapper instance to map the 'Sex' column: 'female' to 0 and 'male' to 1\n",
        "map_sex = Mapper('Sex', {'female': 0, 'male': 1})\n",
        "\n",
        "# Create a Mapper instance to map the 'Embarked' column:\n",
        "# 'S' to 0, 'C' to 1, and 'Q' to 2\n",
        "map_embarked = Mapper('Embarked', {'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# Create a Mapper instance to map the 'Title' column:\n",
        "# 'Mrs' to 4, 'Master' to 3, 'Miss' to 2, 'Mr' to 1, and 'Other' to 0\n",
        "map_title = Mapper('Title', {'Mrs': 4, 'Master': 3, 'Miss': 2, 'Mr': 1, 'Other': 0})"
      ],
      "metadata": {
        "id": "AKE0XRYX8MPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.head()"
      ],
      "metadata": {
        "id": "HriA43Xj9qr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the list of mappers (map_sex, map_embarked, map_title)\n",
        "for i in [map_sex, map_embarked, map_title]:\n",
        "    # Fit the current mapper on the 'data' DataFrame and transform it\n",
        "    # The result is assigned back to 'data1' for each mapper\n",
        "    data1 = i.fit(data).transform(data1)"
      ],
      "metadata": {
        "id": "9xR9Mk3P8z3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1"
      ],
      "metadata": {
        "id": "tISsdNO49Dtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "Wkf_v2rnB330"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **C. Class for Specific operation : Age column transformation**\n",
        "\n",
        "Creating Class for Age column transformation that is compatible with SK_learn pipeline:\n",
        "\n",
        "In the pre-processing steps , a function was created for processing age column. Now we are converting that function into a class suitable for inserting inside the pipeline."
      ],
      "metadata": {
        "id": "31R3mGCtMtQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary modules for creating a custom transformer\n",
        "#from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class age_col_tfr(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Age column transformer\"\"\"\n",
        "\n",
        "    def __init__(self, variables):\n",
        "        \"\"\"\n",
        "        Initialize the transformer with the specified column to be transformed.\n",
        "\n",
        "        Parameters:\n",
        "        variables (str): The name of the column to transform, typically 'Age'.\n",
        "\n",
        "        Raises:\n",
        "        ValueError: If 'variables' is not a string.\n",
        "        \"\"\"\n",
        "        # Ensure that 'variables' is a string (column name)\n",
        "        if not isinstance(variables, str):\n",
        "            raise ValueError('variables should be a str')  # Raise error if not a string\n",
        "        self.variables = variables  # Assign the column name to self.variables\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y=None):\n",
        "        \"\"\"\n",
        "        Fit method (used to calculate the mean and standard deviation of the column to transform).\n",
        "\n",
        "        Parameters:\n",
        "        X (pd.DataFrame): The input data.\n",
        "        y (optional): The target variable, not needed for this transformer.\n",
        "\n",
        "        Returns:\n",
        "        self: The fitted transformer object.\n",
        "        \"\"\"\n",
        "        # Calculate the mean and standard deviation of the 'Age' column\n",
        "        self.age_avg = X[self.variables].mean()  # Mean of the 'Age' column\n",
        "        self.age_std = X[self.variables].std()   # Standard deviation of the 'Age' column\n",
        "\n",
        "        # We need the fit method to make the transformer compatible with sklearn's pipeline\n",
        "        return self  # Return the transformer instance after fitting\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform method (used to fill missing values in the 'Age' column with random values).\n",
        "\n",
        "        Parameters:\n",
        "        X (pd.DataFrame): The input data to be transformed.\n",
        "\n",
        "        Returns:\n",
        "        pd.DataFrame: A DataFrame with missing values in the 'Age' column filled with random values\n",
        "        drawn from a normal distribution.\n",
        "        \"\"\"\n",
        "        np.random.seed(42)  # Set the seed for reproducibility of random values\n",
        "        X = X.copy()  # Create a copy of the input data to avoid altering the original DataFrame\n",
        "\n",
        "        # Count the number of missing values in the 'Age' column\n",
        "        age_null_count = X[self.variables].isnull().sum()\n",
        "\n",
        "        # Generate random values for missing 'Age' values, drawn from a normal distribution\n",
        "        # The random values will be within one standard deviation of the mean\n",
        "        age_null_random_list = np.random.randint(\n",
        "            self.age_avg - self.age_std, self.age_avg + self.age_std, size=age_null_count\n",
        "        )\n",
        "\n",
        "        # Fill the missing values (NaN) in the 'Age' column with the generated random values\n",
        "        X.loc[np.isnan(X[self.variables]), self.variables] = age_null_random_list\n",
        "\n",
        "        # Convert the 'Age' column to integer type (since the random values are generated as integers)\n",
        "        X[self.variables] = X[self.variables].astype(int)\n",
        "\n",
        "        return X  # Return the transformed DataFrame with filled 'Age' values"
      ],
      "metadata": {
        "id": "F3X7hC0kiEye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_tfr=age_col_tfr('Age')"
      ],
      "metadata": {
        "id": "3sv3ZDdLWz4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_tfr.fit(data1)"
      ],
      "metadata": {
        "id": "e4f8JLbJW8_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1=age_tfr.transform(data1)"
      ],
      "metadata": {
        "id": "zdzJw-CvXa7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(data1.Age.isnull())"
      ],
      "metadata": {
        "id": "L-bkS6Kviej2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Building Pipeline**\n",
        "\n",
        "Finally building pipeline and implementing all above class inside pipeline along with classifier also inside."
      ],
      "metadata": {
        "id": "JdJMvVo7T8G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline named 'titanic_pipe' to process and model the Titanic dataset\n",
        "titanic_pipe = Pipeline([\n",
        "\n",
        "    # Embarked column imputation: fill missing values in the 'Embarked' column using the custom 'embarkImputer' class\n",
        "    ('embark_imputation', embarkImputer(variables='Embarked')),\n",
        "\n",
        "    ##========== Mapper: Transform categorical variables to numeric values ==========##\n",
        "\n",
        "    # Map 'Sex' column: Convert 'female' to 0 and 'male' to 1\n",
        "    ('map_sex', Mapper('Sex', {'female': 0, 'male': 1})),\n",
        "\n",
        "    # Map 'Embarked' column: Convert 'S' to 0, 'C' to 1, and 'Q' to 2\n",
        "    ('map_embarked', Mapper('Embarked', {'S': 0, 'C': 1, 'Q': 2})),\n",
        "\n",
        "    # Map 'Title' column: Convert 'Mrs' to 4, 'Master' to 3, 'Miss' to 2, 'Mr' to 1, and 'Other' to 0\n",
        "    ('map_title', Mapper('Title', {'Mrs': 4, 'Master': 3, 'Miss': 2, 'Mr': 1, 'Other': 0})),\n",
        "\n",
        "    # Transform 'Age' column: Fill missing values in the 'Age' column with random values based on the mean and standard deviation\n",
        "    ('age_transform', age_col_tfr(variables='Age')),\n",
        "\n",
        "    # Scale the data: Standardize the features by scaling them to have a mean of 0 and a standard deviation of 1\n",
        "    ('scaler', StandardScaler()),\n",
        "\n",
        "    # Model: Fit a Random Forest Classifier with specified parameters\n",
        "    ('model_rf', RandomForestClassifier(n_estimators=150, max_depth=5, random_state=42))\n",
        "])"
      ],
      "metadata": {
        "id": "MeqaEIn-lMdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train_test_split"
      ],
      "metadata": {
        "id": "RyrrJ0vwseox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x contains all the features except the target variable 'Survived'\n",
        "x = data.drop('Survived', axis=1)\n",
        "\n",
        "# y contains the target variable 'Survived' (survival status)\n",
        "y = data['Survived']\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Display the shapes of the training and testing sets\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "-pV4ja6Dse-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Writing Test cases for checking age_col_tfr**\n",
        "This test sample is kept inside test_features.py. in  production code for testing purpose."
      ],
      "metadata": {
        "id": "y8XQUbHGqbEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try to understand missing index in age column in test set"
      ],
      "metadata": {
        "id": "agpRofVPIpJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "-vYBWm3X-uTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the indices of rows in X_test where the Age column has missing values (NaN)\n",
        "X_test.loc[X_test.Age.isnull(), 'Age'].index"
      ],
      "metadata": {
        "id": "hZ3xSbWNH_LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(X_test.loc[709,'Age'])"
      ],
      "metadata": {
        "id": "Wrl-_DUsISiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using age_tfr class to fill the missing entires"
      ],
      "metadata": {
        "id": "PqjmGIIMIy2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_tfr = age_col_tfr('Age')"
      ],
      "metadata": {
        "id": "AsYwlZE7HuVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#age_tfr.fit(X_train)\n",
        "age_tfr.fit(X_test)"
      ],
      "metadata": {
        "id": "oyKDOPlpJKKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_column_filled = age_tfr.transform(X_test)\n",
        "age_column_filled.loc[709,'Age']"
      ],
      "metadata": {
        "id": "uZr8pnVCIz2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model using pipeline above"
      ],
      "metadata": {
        "id": "yiQ2nL94JjTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the pipeline on the training data (X_train and y_train)\n",
        "titanic_pipe.fit(X_train, y_train)\n",
        "\n",
        "# Use the fitted pipeline to make predictions on the test data (X_test)\n",
        "y_pred = titanic_pipe.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model on the test data\n",
        "print(\"Accuracy(in %):\", accuracy_score(y_test, y_pred) * 100)"
      ],
      "metadata": {
        "id": "4x6Ctgrslpw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persist the model"
      ],
      "metadata": {
        "id": "xgvhU3rRK1px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained pipeline (titanic_pipe) to a file using joblib\n",
        "joblib.dump(titanic_pipe, 'titanic_pip.joblib')"
      ],
      "metadata": {
        "id": "ilfd93a7K0-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for the versions may be used for requirements.txt file"
      ],
      "metadata": {
        "id": "e2tWS2-0q0gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import numpy as np  # For numerical operations and handling arrays\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import sklearn  # For machine learning tasks (includes preprocessing, modeling, etc.)\n",
        "import pydantic  # For data validation and settings management\n",
        "import joblib  # For saving and loading models efficiently"
      ],
      "metadata": {
        "id": "PBWmi8KrrDVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydantic"
      ],
      "metadata": {
        "id": "-FYdEP5ebySl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install strictyaml"
      ],
      "metadata": {
        "id": "WlszwqBUbLig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ruamel.yaml"
      ],
      "metadata": {
        "id": "7luGjUorbaDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import strictyaml"
      ],
      "metadata": {
        "id": "hnMjBIEobO3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ruamel.yaml"
      ],
      "metadata": {
        "id": "kVJtspNLbffK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.__version__)\n",
        "print(pd.__version__)\n",
        "print(sklearn.__version__)\n",
        "print(pydantic.__version__)\n",
        "print(strictyaml.__version__)\n",
        "print(ruamel.yaml.__version__)\n",
        "print(joblib.__version__)"
      ],
      "metadata": {
        "id": "MvuHtoy4sSgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title What is testing? {run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \" \" #@param [\" \", \"The process of creating software from scratch\", \"Identification and fixing software bugs\",\"The practice of ensuring software meets specified requirements and functions as expected\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ]
}