{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitchug/ALMlops/blob/main/M3_AST_02_TextPreprocessing_using_NLTK_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 2: Text Preprocessing using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the use of NLTK library\n",
        "* perform text pre-processing using NLTK such as removing html strips and noise text, removing special characters, lemmatization, stemming, tokenization, removing stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "**NLTK** (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
        "\n",
        "It is a free, open source, community-driven project."
      ],
      "metadata": {
        "id": "okovJktL_Ea7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25xeb09mMs0B"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "The IMDB movie review dataset can be downloaded from [here](http://ai.stanford.edu/~amaas/data/sentiment/). This dataset for binary sentiment classification contains around 50k movie reviews with the following attributes:\n",
        "\n",
        "* **review:** text based review of each movie\n",
        "* **sentiment:** positive or negative sentiment value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdLwhJWI9xmZ"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qggpVfED9xml"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztd1MwbP9xml"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "collapsed": true,
        "outputId": "3b9fc930-f248-4283-da7e-0bcca873a0cd",
        "id": "jCkCqlKf9xml"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=aimlops_c4&recordId=402\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_02_TextPreprocessing_using_NLTK_A\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/IMDB_Dataset.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFl76_ngsasw"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk                                                                        # platform for building Python programs to process natural language\n",
        "nltk.download('stopwords')                                                         # to download the stop words\n",
        "nltk.download('punkt')                                                             # tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences\n",
        "nltk.download('wordnet')                                                           # to lemmatize word using WordNet's built-in function\n",
        "nltk.download('averaged_perceptron_tagger')                                        # for part-of-speech tagger\n",
        "from nltk.corpus import stopwords                                                  # importing the NTLK stopwords to remove articles, preposition and other words that are not actionable\n",
        "from nltk.stem.porter import PorterStemmer                                         # process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
        "from nltk.tokenize import word_tokenize                                            # allows to create individual objects from a bag of words\n",
        "from bs4 import BeautifulSoup                                                      # Python library for pulling data from HTML and XML files\n",
        "import re                                                                          # regular expression (or RE) specifies a set of strings that matches it\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7bG2m775ba6"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_fAB1U0fqJ7"
      },
      "outputs": [],
      "source": [
        "# read the dataset\n",
        "df = # YOUR CODE HERE to read 'IMDB_Dataset.csv'\n",
        "print(df.shape)\n",
        "df.head(10)      # first 10 rows"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us view one of the reviews\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "sH2F7KdgLXMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy2WsjbRDCJr"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8mLwZe5CWSN"
      },
      "outputs": [],
      "source": [
        "# summary of the dataset\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twI5ntekDNrs"
      },
      "source": [
        "Now, we will look at the sentiment count by category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6izwMrUVDQ6f"
      },
      "outputs": [],
      "source": [
        "# sentiment count category-wise\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the postive and negative sentiments in a bar plot\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Z6HG5xcMCxJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyeDUn36DVRn"
      },
      "source": [
        "We can see that the dataset is balanced.\n",
        "\n",
        "Now, we will do the text cleaning of the reviews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on2XCsxtI0-b"
      },
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "The data scraped from the website is mostly in the raw text form. This data needs to be cleaned before analyzing it or fitting a model to it. Cleaning up the text data is necessary to highlight the attributes that we are going to want our machine learning system to pick up on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tupdRoUSGYrw"
      },
      "source": [
        "**Removing noisy text**\n",
        "\n",
        "Sample noise removal tasks could include:\n",
        "\n",
        "* removing text file headers, footers\n",
        "* removing HTML, XML, etc. markup and metadata\n",
        "* extracting valuable data from other formats, such as JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzsW7umy6Sea"
      },
      "outputs": [],
      "source": [
        "# removing the html strips\n",
        "def strip_html(text):\n",
        "    # BeautifulSoup is a useful library for extracting data from HTML and XML documents\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    text = text.replace('[', '')\n",
        "    # YOUR CODE HERE to remove ']'\n",
        "    return text\n",
        "    #return re.sub('\\[[^]]*\\]', '', text)          # to remove the text also present within square brackets\n",
        "\n",
        "# removing the noisy text\n",
        "def denoise_text(text):\n",
        "    # YOUR CODE HERE to update the text by applying strip_html() function\n",
        "    # YOUR CODE HERE to update the text by applying remove_between_square_brackets() function\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"\"\"<body>\n",
        "<h1>This is a test heading.</h1>\n",
        "<p>Starting a paragraph here...</p>\n",
        "</body>\"\"\"\n",
        "\n",
        "# YOUR CODE HERE to call strip_html() on txt"
      ],
      "metadata": {
        "id": "9Mt02JQqEqzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"I bought [apple, banana, orange] yesterday.\"\n",
        "\n",
        "# YOUR CODE HERE to call remove_between_square_brackets() on txt"
      ],
      "metadata": {
        "id": "rEYyDYVGFmiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x56E-c6KFvZH"
      },
      "outputs": [],
      "source": [
        "# apply denoise_text() function on review column\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQzlTJ-VGwVu"
      },
      "source": [
        "**Removing special characters**\n",
        "\n",
        "Special characters typically include any character that is not a letter or number, such as punctuation. Removing special characters from a string results in a string containing only letters and numbers.\n",
        "\n",
        "We can use the `re` python library for Regular expression operations.\n",
        "\n",
        "To know more about Regular expressions, click [here](https://realpython.com/regex-python/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsi1MZ9X6Xqm"
      },
      "outputs": [],
      "source": [
        "# define function for removing special characters\n",
        "def remove_special_characters(text):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Hi There! How are you?\"\n",
        "\n",
        "# YOUR CODE HERE to call remove_special_characters() on txt"
      ],
      "metadata": {
        "id": "acd8t0kuIDOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBWV2c7oGpVL"
      },
      "outputs": [],
      "source": [
        "# apply remove_special_characters() function on review column\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKHDo92_nm-H"
      },
      "source": [
        "**Lemmatization**\n",
        "\n",
        "Lemmatization is a text pre-processing technique used to break a word down to its root meaning or word (called lemma) to identify similarities.\n",
        "\n",
        "For example, a lemmatization algorithm would reduce the word ***better*** to its root word, or lemme, ***good***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjqMtkJ9ake5"
      },
      "outputs": [],
      "source": [
        "# Lemmatize word using WordNet's built-in function\n",
        "# pos: The Part Of Speech tag.\n",
        "#      Valid options are \"n\" for nouns, \"v\" for verbs, \"a\" for adjectives,\n",
        "#                        \"r\" for adverbs and \"s\" for satellite adjectives.\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\", ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGsYJGmLG9bM"
      },
      "source": [
        "**Text Stemming**\n",
        "\n",
        "Stemming, also called suffix stripping, is a technique used to reduce text dimensionality. Stemming is also a type of text normalization that enables you to standardize some words into specific expressions also called stems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R8eAA_b6bOS"
      },
      "outputs": [],
      "source": [
        "# stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"He likes lemons bananas and oranges. He goes to market.\"\n",
        "\n",
        "# YOUR CODE HERE to call simple_stemmer() on txt"
      ],
      "metadata": {
        "id": "63MTPhVdIw45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmBWpKUhG5F7"
      },
      "outputs": [],
      "source": [
        "# apply function on review column\n",
        "# YOUR CODE HERE to apply 'simple_stemmer' function on reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA2RsIUenbGe"
      },
      "source": [
        "**Tokenization**\n",
        "\n",
        "Tokenization is the process of splitting paragraphs and sentences into smaller understandable parts (words).\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbCqVxg_F6jw"
      },
      "outputs": [],
      "source": [
        "tokens = word_tokenize('I enjoy playing football in the rain.')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part-of-speech tagging**\n",
        "\n",
        "Part-of-speech (POS) tagging is the task of determining the word class of a token. This is crucial for *disambiguation*, because different parts of speech may have similar forms."
      ],
      "metadata": {
        "id": "BvcVK-EfOnGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged = nltk.pos_tag(tokens)\n",
        "tagged"
      ],
      "metadata": {
        "id": "vuT3NelVN44l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omeXdqsFHm1M"
      },
      "source": [
        "**Removing stop words**\n",
        "\n",
        "Stop words are English words that do not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgIjbfZgEgNH"
      },
      "outputs": [],
      "source": [
        "# setting english stopwords\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "print(stopword_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jacr7HAjfILU"
      },
      "source": [
        "The above list of stopwords also contains the word \"not\", and its other forms such as don't, didn't, etc. We need them for correct sentiment classification.\n",
        "\n",
        "For example, consider a negative review \"*not a good movie*\", and if we remove 'not' from it then it becomes a positive review \"*a good movie*\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1uSxBepgdsE"
      },
      "outputs": [],
      "source": [
        "# Exclude 'not' and its other forms from the stopwords list\n",
        "\n",
        "updated_stopword_list = []\n",
        "\n",
        "for word in stopword_list:\n",
        "    if word=='not' or word.endswith(\"n't\"):\n",
        "        pass\n",
        "    else:\n",
        "        updated_stopword_list.append(word)\n",
        "\n",
        "print(updated_stopword_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fH5HXfk8OJj"
      },
      "outputs": [],
      "source": [
        "# removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    # splitting strings into tokens (list of words)\n",
        "\n",
        "    # YOUR CODE HERE to create 'tokens' variable by tokenizing 'text'\n",
        "\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        # filtering out the stop words\n",
        "        filtered_tokens = [token for token in tokens if token not in updated_stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in updated_stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"The movie was not that great\"\n",
        "\n",
        "# YOUR CODE HERE to call remove_stopwords() on txt"
      ],
      "metadata": {
        "id": "zVGniBcAKyvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_OBq2crHXU5"
      },
      "outputs": [],
      "source": [
        "# apply remove_stopwords function on review column\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title What is the primary goal of tokenization in natural language processing? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"To classify text into categories\", \"To split text into individual units such as words or phrases\", \"To generate synthetic text data\", \"To translate text into different languages\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ym68JPOvHPm"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr_w2auyvHPn"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRwFaM1DvHPo"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihp2ejlNvHPo"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbJ6ik8mvHPp"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "N2DKhsDNvHPp"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}